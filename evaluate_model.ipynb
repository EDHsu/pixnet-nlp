{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "import re, jieba\n",
    "import gensim\n",
    "import difflib\n",
    "import numpy as np\n",
    "import random\n",
    "import opencc\n",
    "from smart_open import smart_open\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "ANS = ['a', 'b', 'c', 'd', 'e']\n",
    "WINDOW = 10\n",
    "VEC_SIZE = 100\n",
    "def simple_preprocess(content):\n",
    "    content = content.strip().replace('︽⊙＿⊙︽', '龐燮傍謝')\n",
    "    wlist = list(jieba.cut(content))\n",
    "    qidx = []\n",
    "    i = 0\n",
    "    for w in wlist:\n",
    "        if w == '龐燮傍謝':\n",
    "            wlist[i] = '*'\n",
    "            qidx.append(i)\n",
    "        i += 1\n",
    "    return (wlist, qidx)\n",
    "\n",
    "def normalize_vec(vec):\n",
    "    mag = ((vec * vec).sum()) ** 0.5\n",
    "    return vec / mag\n",
    "\n",
    "def build_estimate_samples(wlist, qidx):\n",
    "    global WINDOW\n",
    "    temp = wlist[:]\n",
    "    est_sen = []\n",
    "    sen_len = len(wlist)\n",
    "    for i in qidx:\n",
    "        head = max(i - WINDOW, 0)\n",
    "        tail = min(i + WINDOW, sen_len)\n",
    "        est_sen.append(wlist[head : i] + wlist[i + 1 : tail])\n",
    "    return est_sen\n",
    "\n",
    "def estimate_ans(est_sen_list, options):\n",
    "    global VEC_SIZE\n",
    "    n_sen = float(len(est_sen_list))\n",
    "    option_vec_idx = []\n",
    "    for w in options:\n",
    "        if w in model: option_vec_idx.append(model.vocab[w].index)\n",
    "        else: \n",
    "            option_vec_idx.append(-1)\n",
    "            return -1\n",
    "    score = [0., 0., 0., 0., 0.]\n",
    "    for wlist in est_sen_list:\n",
    "        arr = np.zeros(VEC_SIZE)\n",
    "        for w in wlist:\n",
    "            if w in model and w != u'*': arr += model[w]\n",
    "        for i in range(5):\n",
    "#             if option_vec_idx[i] >= 0: score[i] += np.dot(normalize_vec(arr), \n",
    "#                                                           normalize_vec(model[model.index2word[option_vec_idx[i]]]))\n",
    "            if option_vec_idx[i] >= 0: score[i] += np.dot(arr, \n",
    "                                                          syn1neg[option_vec_idx[i]])\n",
    "    \n",
    "    for i in range(5):\n",
    "        score[i] /= n_sen\n",
    "    \n",
    "    return ANS[score.index(max(score))]\n",
    "#     np.dot(arr, syn1neg[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jieba.add_word('龐燮傍謝', freq=10, tag='xx')\n",
    "# jieba.load_userdict('data/zhwiki-cn-clean')\n",
    "# jieba.load_userdict('data/dict-txt-big')\n",
    "\n",
    "path = 'w2v-experiment/model/'\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(path + 'sk-syn0.bin', binary = True)\n",
    "vocab_size, vector_size = model.syn0.shape\n",
    "syn1neg = np.zeros((vocab_size, vector_size), dtype=np.float32)\n",
    "binary_len = np.dtype(np.float32).itemsize * vector_size\n",
    "with smart_open(path + 'sk-syn1neg.bin') as fin:\n",
    "    for i in range(vocab_size):\n",
    "        weights = np.fromstring(fin.read(binary_len), dtype=np.float32)\n",
    "        syn1neg[i] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7633587786259542\n"
     ]
    }
   ],
   "source": [
    "right, wrong = 0, 0\n",
    "wrong_sen = []\n",
    "with open('question_official/hackathon_1000_cn.tsv', 'rb') as f:\n",
    "    for i in range(200): next(f)\n",
    "    for line in f:\n",
    "#         s = opencc.convert(line.decode('utf-8'), config = '/usr/share/opencc/tw2s.json')\n",
    "        s = line.decode('utf-8')\n",
    "#         print(repr(s))\n",
    "        parse = s.split('\\t')\n",
    "#         print(parse)\n",
    "        no = int(parse[0])\n",
    "        content = parse[1].strip()\n",
    "        ans = parse[2]\n",
    "        options = parse[3 : 8]\n",
    "        ans_ref = parse[8]\n",
    "        url = parse[9]\n",
    "        level = parse[10]\n",
    "#         print(content)\n",
    "#         print(options, ans_ref)\n",
    "        wlist, qidx = simple_preprocess(content)\n",
    "        est_sen = build_estimate_samples(wlist, qidx)\n",
    "        pred = estimate_ans(est_sen, options)\n",
    "        if pred == -1:\n",
    "#             wrong += 1\n",
    "            continue\n",
    "        if pred == ans: right += 1\n",
    "        else: \n",
    "            wrong += 1\n",
    "            wrong_sen.append((content, options[ANS.index(pred)], ans_ref))\n",
    "#         break\n",
    "print(float(right)/float(right+wrong))\n",
    "#  (['脸上', '比较', '油', '的', '时候', '我', '都', '会', '习惯', '让', '*', '多', '停留', '个', '10', \n",
    "#    '秒左右', '，', '会', '比', '马上', '就', '洗掉', '清洁', '得', '更', '干净', '一点', '唷'], [10])\n",
    "# v100w10n50p10i1 : 0.764065335753176\n",
    "# v100w10n50p10i10 : 0.7985480943738656\n",
    "# v100w10n50p10i30 : 0.8076225045372051\n",
    "# v100w20n50p10i10 : 0.79491833030853\n",
    "# v80w5n50p10i30 : 0.7531760435571688\n",
    "# v200w10n50p10i10 : 0.8148820326678766\n",
    "# v200w10n50p10i50 : 0.7912885662431942\n",
    "# v200w10n50p10i30 : 0.7586206896551724\n",
    "# v100w10n50p10i10hs1 : 0.7967332123411979\n",
    "# v100w10n50p1i10 : 0.79491833030853\n",
    "\n",
    "# cbow: 0.8137404580152672\n",
    "# skgram: 0.7633587786259542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('松山文创', 0.7016356587409973),\n",
       " ('南港软体', 0.6479178667068481),\n",
       " ('旁Wiz', 0.6130987405776978),\n",
       " ('Waffogato', 0.5951836705207825),\n",
       " ('@内科', 0.5815931558609009),\n",
       " ('内湖', 0.5726386904716492),\n",
       " ('南港', 0.5714350938796997),\n",
       " ('@捷运港墘站', 0.5695842504501343),\n",
       " ('榆小', 0.5559353828430176),\n",
       " ('松烟', 0.5554336309432983)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['松烟文创'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = ' '.join(list(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台艺大文创\n",
      "文创区里\n",
      "青青文创\n",
      "花莲文创产业\n",
      "台北文创\n",
      "文创产业\n",
      "文创\n",
      "十鼓文创\n",
      "著文创\n",
      "猫头鹰文创\n",
      "文创生活\n",
      "成文创\n",
      "起初文创\n",
      "经贸文创\n",
      "灿星文创\n",
      "华山文创\n",
      "学学文创\n",
      "台北文创大楼\n",
      "图写文创\n",
      "文创头围\n",
      "文创聚落\n",
      "谷物文创\n",
      "头围文创\n",
      "文创工作者\n",
      "梦田文创\n",
      "松烟文创\n",
      "美地文创\n",
      "拥恒文创\n",
      "莎贝莉娜文创\n",
      "文创产业园区\n",
      "丰味文创\n",
      "甘乐文创\n",
      "松山文创\n",
      "文创化\n",
      "华山文创产业\n",
      "文创园区\n",
      "文创舘\n"
     ]
    }
   ],
   "source": [
    "for match in re.findall(r'\\s(\\S*文创\\S*)\\s', word_dict): \n",
    "    print(match)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975332"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model[model.index2word[1]]\n",
    "vocab_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
