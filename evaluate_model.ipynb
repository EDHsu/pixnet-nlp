{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "import re, jieba\n",
    "import gensim\n",
    "import difflib\n",
    "import numpy as np\n",
    "import random\n",
    "import opencc\n",
    "from smart_open import smart_open\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "ANS = ['a', 'b', 'c', 'd', 'e']\n",
    "FEATURES = ['no', 'w_idx', 'word', 'cos_ref','cos_syn1', 'cos_syn0', 'dist_syn0', 'target']\n",
    "WINDOW = 10\n",
    "VEC_SIZE = 100\n",
    "def simple_preprocess(content):\n",
    "    content = content.strip().replace('︽⊙＿⊙︽', '龐燮傍謝')\n",
    "    wlist = list(jieba.cut(content))\n",
    "    qidx = []\n",
    "    i = 0\n",
    "    for w in wlist:\n",
    "        if w == '龐燮傍謝':\n",
    "            wlist[i] = '*'\n",
    "            qidx.append(i)\n",
    "        i += 1\n",
    "    return (wlist, qidx)\n",
    "\n",
    "def normalize_vec(vec):\n",
    "    mag = ((vec * vec).sum()) ** 0.5\n",
    "    return vec / mag\n",
    "\n",
    "def build_estimate_samples(wlist, qidx):\n",
    "    global WINDOW\n",
    "    temp = wlist[:]\n",
    "    est_sen = []\n",
    "    sen_len = len(wlist)\n",
    "    for i in qidx:\n",
    "        head = max(i - WINDOW, 0)\n",
    "        tail = min(i + WINDOW, sen_len)\n",
    "        est_sen.append(wlist[head : i] + wlist[i + 1 : tail])\n",
    "    return est_sen\n",
    "\n",
    "def estimate_ans(est_sen_list, options, model, syn1neg):\n",
    "    global VEC_SIZE\n",
    "    n_sen = float(len(est_sen_list))\n",
    "    option_vec_idx = []\n",
    "    for w in options:\n",
    "        if w in model: option_vec_idx.append(model.vocab[w].index)\n",
    "        else: \n",
    "            option_vec_idx.append(-1)\n",
    "            return -1\n",
    "    score = [0., 0., 0., 0., 0.]\n",
    "    for wlist in est_sen_list:\n",
    "        arr = np.zeros(VEC_SIZE)\n",
    "        for w in wlist:\n",
    "            if w in model and w != u'*': arr += model[w]\n",
    "        for i in range(5):\n",
    "            if option_vec_idx[i] >= 0: \n",
    "#                 score[i] += np.dot(normalize_vec(arr), normalize_vec(model[model.index2word[option_vec_idx[i]]]))\n",
    "                score[i] += np.dot(normalize_vec(arr), normalize_vec(syn1neg[option_vec_idx[i]]))\n",
    "#                 score[i] += np.dot(arr, syn1neg[option_vec_idx[i]])\n",
    "    \n",
    "    for i in range(5):\n",
    "        score[i] /= n_sen\n",
    "    \n",
    "    return ANS[score.index(max(score))]\n",
    "#     np.dot(arr, syn1neg[idx])\n",
    "\n",
    "\n",
    "def generate_feature(no, w_list, opt_list, ans, syn0_model, syn1, prefix):\n",
    "    # input sample: \n",
    "    # w_list = ['高雄','转','144','次','自强号','1700','高雄','开','1923','到','1940','到','台北']\n",
    "    # opt_list = ['两用', '阿明', '员林', '碎屑', '精力']\n",
    "    # ans = 'c'\n",
    "    # prefix = 'cbow'\n",
    "    opt_num = len(opt_list)\n",
    "    ans = opt_list[ANS.index(ans)]  # ans: 'c' --> '员林'\n",
    "    hidd_vec = np.zeros(VEC_SIZE)\n",
    "    for w in w_list:\n",
    "        if w in syn0_model and w != u'*': hidd_vec += syn0_model[w]\n",
    "    feats = []\n",
    "    for w in opt_list:\n",
    "        if w in syn0_model: \n",
    "            w_idx = syn0_model.vocab[w].index\n",
    "            cos_ref = np.dot(syn0_model[w], syn1[w_idx])\n",
    "#             cos_syn1 = np.dot(hidd_vec, syn1[w_idx])\n",
    "            cos_syn1 = np.dot(normalize_vec(hidd_vec), normalize_vec(syn1[w_idx]))\n",
    "#             cos_syn0 = np.dot(hidd_vec, syn0_model[w])\n",
    "            cos_syn0 = np.dot(normalize_vec(hidd_vec), normalize_vec(syn0_model[w]))\n",
    "            dist_syn0 = sum((hidd_vec - syn0_model[w]) ** 2)\n",
    "            if w == ans:\n",
    "                feats.append([no, w_idx, w, cos_ref, cos_syn1, cos_syn0, dist_syn0, 1])\n",
    "            else:\n",
    "                feats.append([no, w_idx, w, cos_ref, cos_syn1, cos_syn0, dist_syn0, 0])\n",
    "        else:\n",
    "            pass\n",
    "    df = pd.DataFrame(feats, columns=[prefix + '_' + f for f in FEATURES])\n",
    "    cols_to_norm = [prefix + '_' + f for f in FEATURES[4:-1]]\n",
    "    df[cols_to_norm] = (df[cols_to_norm] - df[cols_to_norm].mean()) / df[cols_to_norm].std()\n",
    "#     df[cols_to_norm] = (df[cols_to_norm] - df[cols_to_norm].min()) / (df[cols_to_norm].max() - df[cols_to_norm].min())\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_model(path, prefix):\n",
    "    # input sample:\n",
    "    # path = 'w2v-experiment/model/'\n",
    "    # prefix = 'sk'\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(path + prefix + '-syn0.bin', binary = True)\n",
    "    vocab_size, vector_size = model.syn0.shape\n",
    "    syn1neg = np.zeros((vocab_size, vector_size), dtype=np.float32)\n",
    "    binary_len = np.dtype(np.float32).itemsize * vector_size\n",
    "    with smart_open(path + prefix + '-syn1neg.bin') as fin:\n",
    "        for i in range(vocab_size):\n",
    "            weights = np.fromstring(fin.read(binary_len), dtype=np.float32)\n",
    "            syn1neg[i] = weights\n",
    "    return (model, syn1neg)\n",
    "\n",
    "def load_sample(nth_line):\n",
    "    with open('question_official/hackathon_1000_cn.tsv', 'rb') as f:\n",
    "        for i in range(nth_line): next(f)\n",
    "        for line in f:\n",
    "            s = line.decode('utf-8')\n",
    "            parse = s.split('\\t')\n",
    "            no = int(parse[0])\n",
    "            content = parse[1].strip()\n",
    "            ans = parse[2]\n",
    "            opt_list = parse[3 : 8]\n",
    "            ans_ref, url, level = parse[8], parse[9], parse[10]\n",
    "            wlist, qidx = simple_preprocess(content)\n",
    "            sen_list = build_estimate_samples(wlist, qidx)\n",
    "            break\n",
    "        return (sen_list, opt_list, ans)\n",
    "\n",
    "def sum_word_vec(w_list, syn0_model):\n",
    "    hidd_vec = np.zeros(VEC_SIZE)\n",
    "    for w in w_list:\n",
    "        if w in syn0_model and w != u'*': hidd_vec += syn0_model[w]\n",
    "    return hidd_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/4_/b_9vfbwx41g4t48jrzr0r4vm0000gn/T/jieba.cache\n",
      "Loading model cost 1.077 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# jieba.add_word('龐燮傍謝', freq=10, tag='xx')\n",
    "# jieba.load_userdict('data/zhwiki-cn-clean')\n",
    "# jieba.load_userdict('data/dict-txt-big')\n",
    "# path = 'w2v-experiment/model/'\n",
    "# cbow_model, cbow_syn1neg = load_model(path, 'cbow')\n",
    "# sk_model, sk_syn1neg = load_model(path, 'sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726, 12)\n"
     ]
    }
   ],
   "source": [
    "overall_df = pd.DataFrame()\n",
    "with open('question_official/hackathon_1000_cn.tsv', 'rb') as f:\n",
    "    for line in f:\n",
    "        s = line.decode('utf-8')\n",
    "        parse = s.split('\\t')\n",
    "        no = int(parse[0])\n",
    "        content = parse[1].strip()\n",
    "        ans = parse[2]\n",
    "        opt_list = parse[3 : 8]\n",
    "        ans_ref, url, level = parse[8], parse[9], parse[10]\n",
    "        wlist, qidx = simple_preprocess(content)\n",
    "        sen_list = build_estimate_samples(wlist, qidx)\n",
    "        \n",
    "        for w_list in sen_list:\n",
    "            cbow_df = generate_feature(no, w_list, opt_list, ans, cbow_model, cbow_syn1neg, 'cbow')\n",
    "            cbow_df.drop(['cbow_target', 'cbow_no', 'cbow_word'], axis=1, inplace=True)\n",
    "            cbow_df.rename(columns = {'cbow_w_idx':'w_idx'}, inplace = True)\n",
    "            sk_df = generate_feature(no, w_list, opt_list, ans, sk_model, sk_syn1neg, 'sk')\n",
    "            sk_df.drop(['sk_w_idx'], axis=1, inplace=True)\n",
    "            sk_df.rename(columns = {'sk_target':'target'}, inplace = True)\n",
    "            df = pd.concat([cbow_df, sk_df], axis = 1)\n",
    "            overall_df = pd.concat([overall_df, df], axis = 0, ignore_index=True)\n",
    "\n",
    "print(overall_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['虽然', '青年', '活动中心', '的', '建筑', '光影', '很漂亮', '，', '不过', '我们']\n",
      "['暮色', '广宣', '天祥', '原址', '步道']\n"
     ]
    }
   ],
   "source": [
    "sen_list, opt_list, ans = load_sample(994)\n",
    "print(sen_list[0])\n",
    "print(opt_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.75660098  0.44147068  0.0935495   0.05515734  0.58966267 -0.1954944\n",
      "  0.23016895  0.0775224   0.67222166  0.60646921  0.42620423  0.05359171\n",
      " -0.35594633 -0.00262699 -0.39729723  0.31234422 -0.2772193   0.34083471\n",
      " -0.57637209  0.17717554]\n",
      "[-0.7013362   0.3646695   0.17631839 -1.32972857  0.93278129  1.15400534\n",
      "  0.82759415  0.8628505   1.23306865  1.44381509  0.46014024  0.06557306\n",
      " -1.84071061  0.12600059 -0.01049444  0.09357804 -0.6063582   1.00286718\n",
      " -0.50865764 -0.60646717]\n",
      "74.351392936\n"
     ]
    }
   ],
   "source": [
    "w = '步道'\n",
    "w_idx = cbow_model.vocab[w].index\n",
    "vec = sum_word_vec(sen_list[0], cbow_model)\n",
    "print(cbow_model[w][:20])\n",
    "print(vec[:20])\n",
    "print(sum(abs(vec - cbow_model[w])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# overall_df.tail(30)\n",
    "overall_df.to_csv('question_official/overall_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right, wrong = 0, 0\n",
    "wrong_sen = []\n",
    "with open('question_official/hackathon_1000_cn.tsv', 'rb') as f:\n",
    "#     for i in range(200): next(f)\n",
    "    for line in f:\n",
    "#         s = opencc.convert(line.decode('utf-8'), config = '/usr/share/opencc/tw2s.json')\n",
    "        s = line.decode('utf-8')\n",
    "#         print(repr(s))\n",
    "        parse = s.split('\\t')\n",
    "#         print(parse)\n",
    "        no = int(parse[0])\n",
    "        content = parse[1].strip()\n",
    "        ans = parse[2]\n",
    "        options = parse[3 : 8]\n",
    "        ans_ref = parse[8]\n",
    "        url = parse[9]\n",
    "        level = parse[10]\n",
    "#         print(content)\n",
    "#         print(options, ans_ref)\n",
    "        wlist, qidx = simple_preprocess(content)\n",
    "        est_sen = build_estimate_samples(wlist, qidx)\n",
    "        pred = estimate_ans(est_sen, options, sk_model, sk_syn1neg)\n",
    "        if pred == -1:\n",
    "#             wrong += 1\n",
    "            continue\n",
    "        if pred == ans: right += 1\n",
    "        else: \n",
    "            wrong += 1\n",
    "            wrong_sen.append((content, options[ANS.index(pred)], ans_ref))\n",
    "#         break\n",
    "print(float(right)/float(right+wrong))\n",
    "\n",
    "\n",
    "#  (['脸上', '比较', '油', '的', '时候', '我', '都', '会', '习惯', '让', '*', '多', '停留', '个', '10', \n",
    "#    '秒左右', '，', '会', '比', '马上', '就', '洗掉', '清洁', '得', '更', '干净', '一点', '唷'], [10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### v100w10n50p10i1 : 0.764065335753176\n",
    "### v100w10n50p10i10 : 0.7985480943738656\n",
    "### v100w10n50p10i30 : 0.8076225045372051\n",
    "### v100w20n50p10i10 : 0.79491833030853\n",
    "### v80w5n50p10i30 : 0.7531760435571688\n",
    "### v200w10n50p10i10 : 0.8148820326678766\n",
    "### v200w10n50p10i50 : 0.7912885662431942\n",
    "### v200w10n50p10i30 : 0.7586206896551724\n",
    "### v100w10n50p10i10hs1 : 0.7967332123411979\n",
    "### v100w10n50p1i10 : 0.79491833030853\n",
    "\n",
    "### cbow: 0.8137404580152672\n",
    "### skgram: 0.7633587786259542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_df = pd.DataFrame()\n",
    "with open('question_official/hackathon_1000_cn.tsv', 'rb') as f:\n",
    "    for line in f:\n",
    "        s = line.decode('utf-8')\n",
    "        parse = s.split('\\t')\n",
    "        no = int(parse[0])\n",
    "        content = parse[1].strip()\n",
    "        ans = parse[2]\n",
    "        opt_list = parse[3 : 8]\n",
    "        ans_ref, url, level = parse[8], parse[9], parse[10]\n",
    "        wlist, qidx = simple_preprocess(content)\n",
    "        sen_list = build_estimate_samples(wlist, qidx)\n",
    "        \n",
    "        for w_list in sen_list:\n",
    "            cbow_df = generate_feature(no, w_list, opt_list, ans, cbow_model, cbow_syn1neg, 'cbow')\n",
    "            cbow_df.drop(['cbow_target', 'cbow_no', 'cbow_word'], axis=1, inplace=True)\n",
    "            cbow_df.rename(columns = {'cbow_w_idx':'w_idx'}, inplace = True)\n",
    "            sk_df = generate_feature(no, w_list, opt_list, ans, sk_model, sk_syn1neg, 'sk')\n",
    "            sk_df.drop(['sk_w_idx'], axis=1, inplace=True)\n",
    "            sk_df.rename(columns = {'sk_target':'target'}, inplace = True)\n",
    "            df = pd.concat([cbow_df, sk_df], axis = 1)\n",
    "            overall_df = pd.concat([overall_df, df], axis = 0, ignore_index=True)\n",
    "\n",
    "print(overall_df.shape)\n",
    "\n",
    "# debug = pd.DataFrame()\n",
    "# w_list = ['高雄','转','144','次','自强号','1700','高雄','开','1923','到','1940','到','台北']\n",
    "# opt_list = ['两用', '阿明', '员林', '碎屑', '精力']\n",
    "# ans = 'c'\n",
    "# cbow_df = generate_feature(w_list, opt_list, ans, cbow_model, cbow_syn1neg, 'cbow')\n",
    "# cbow_df.drop(['cbow_target'], axis=1, inplace=True)\n",
    "# cbow_df.rename(columns = {'cbow_w_idx':'w_idx'}, inplace = True)\n",
    "# sk_df = generate_feature(w_list, opt_list, ans, sk_model, sk_syn1neg, 'sk')\n",
    "# sk_df.drop(['sk_w_idx'], axis=1, inplace=True)\n",
    "# sk_df.rename(columns = {'sk_target':'target'}, inplace = True)\n",
    "# x = pd.concat([cbow_df, sk_df], axis = 1)\n",
    "# pd.concat([debug, x], axis = 0, ignore_index=True)\n",
    "# # print(cbow_df)\n",
    "# # print(sk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(['松烟文创'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = ' '.join(list(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for match in re.findall(r'\\s(\\S*文创\\S*)\\s', word_dict): \n",
    "    print(match)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-54ab90609297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model[model.index2word[1]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# model[model.index2word[1]]\n",
    "vocab_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
